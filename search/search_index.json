{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>network-routing</code>","text":""},{"location":"#about","title":"About","text":"<p>This is the documentation for <code>network_routing</code>, a Python package developed by DVRPC to produce the datasets shown in the Sidewalk Gap Analysis Explorer.</p> <p>This work was conducted using DVRPC\u2019s pedestrian facilities inventory, a GIS dataset that inventories sidewalks, curb ramps, and crosswalks across the nine-county Greater Philadelphia region. Explore the data and help plan for a pedestrian-friendly future at walk.dvrpc.org.</p> <p>This codebase facilitates the creation and analysis of routable networks, using any topological network dataset including:</p> <ul> <li>OpenStreetMap</li> <li>DVRPC's sidewalk network</li> <li>DVRPC's Level of Traffic Stress network</li> </ul> <p>Data for the analysis is stored in a <code>PostgreSQL/PostGIS</code> database and the process is scripted using <code>Python</code>.</p> <p>The codebase is broken up into four primary modules, each with its own command-line-interface (CLI):</p> <ul> <li><code>db</code> controls all data I/O</li> <li><code>access</code> manages all accessibility/routing analyses</li> <li><code>gaps</code> handles analysis processes that don't use network routing</li> <li><code>improvements</code> generates tables showing the missing gaps in the sidewalk network</li> </ul> <p>It builds on top of a variety of libraries, including <code>pandana</code>, <code>pg_data_etl</code>, <code>osmnx</code>, <code>geopandas</code>/<code>pandas</code>, <code>geoalchemy2</code>/<code>sqlalchemy</code>, &amp; <code>psycopg2</code></p>"},{"location":"#software-requirements","title":"Software Requirements","text":"<p>This codebase requires the following software:</p> <ul> <li><code>git</code></li> <li>Python 3</li> <li>PostgreSQL &amp; PostGIS</li> </ul> <p>Optional software includes:</p> <ul> <li><code>make</code> if you'd like to take advantage of the bundled sets of commands defined in the <code>Makefile</code></li> <li><code>tippecanoe</code> is needed if you'd like to generate vector tile outputs for a webmap</li> </ul>"},{"location":"cloud/","title":"Running this analysis remotely","text":"<p>If you're having trouble with a Windows computer or otherwise don't have the compute power locally necessary for this analysis, you can run this code in the cloud with following these steps.</p>"},{"location":"cloud/#create-a-droplet-on-digital-ocean","title":"Create a \"Droplet\" on Digital Ocean","text":"<p>In your Digital Ocean account, create a Virtual Machine (\"Droplet\") with enough resources to run this analysis. This tutorial was tested using the following specs:</p> <ul> <li>64 GB RAM</li> <li>8 Intel vCPUs</li> <li>160 GB NVME SSD disk</li> <li>Ubuntu 20.04 (LTS) x64</li> </ul> <p>Follow Digital Ocean's initial setup instructions to create a non-<code>root</code> user with <code>sudo</code> permissions.</p>"},{"location":"cloud/#install-dependencies","title":"Install dependencies","text":"<p>Connect to your droplet and install the following:</p>"},{"location":"cloud/#miniconda","title":"Miniconda","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"cloud/#postgresql-postgis","title":"PostgreSQL / PostGIS","text":"<pre><code>sudo apt update\nsudo apt install postgis\n</code></pre>"},{"location":"cloud/#make-tippecanoe","title":"make / tippecanoe","text":"<pre><code>sudo apt install make\nsudo apt-get install build-essential libsqlite3-dev zlib1g-dev\ngit clone https://github.com/mapbox/tippecanoe.git\ncd tippecanoe\nmake -j\nsudo make install\n</code></pre>"},{"location":"cloud/#prepare-for-analysis","title":"Prepare for analysis","text":""},{"location":"cloud/#clone-the-network-routing-repo-from-github","title":"Clone the <code>network-routing</code> repo from GitHub","text":"<pre><code>git clone https://github.com/dvrpc/network-routing.git\n</code></pre>"},{"location":"cloud/#create-the-conda-environment","title":"Create the <code>conda</code> environment","text":"<pre><code>cd network-routing\nconda env create -f environment.yml\n</code></pre>"},{"location":"cloud/#use-filezilla-to-upload-the-necessary-shapefiles-via-ftp","title":"Use Filezilla to upload the necessary shapefiles via FTP","text":"<p>Instead of connecting directly to the shared folder via GoogleDrive, you'll need to copy the necessary shapefiles via FTP. Follow these directions to set up the connection, and once you're in create the following folder tree:</p> <pre><code>/home/sammy/GDrive-Mirror/Shared drives/network-routing-repo-data/data\n</code></pre> <p>and then copy the entire <code>inputs</code> folder over.</p>"},{"location":"cloud/#create-the-postgis-database","title":"Create the PostGIS database","text":"<p>Connect to the root database to create a new one for this analysis:</p> <pre><code>sudo -u postgres psql\n</code></pre> <pre><code>postgres=# CREATE DATABASE example_db;\n</code></pre> <p>Then connect to the new database and enable PostGIS:</p> <pre><code>sudo -u postgres psql -d example_db\n</code></pre> <pre><code>example_db=# CREATE EXTENSION postgis;\n</code></pre>"},{"location":"cloud/#configure-the-env-file","title":"Configure the <code>.env</code> file","text":"<p>At the root level of the cloned repo, create a file with the following:</p> <pre><code>DATABASE_URL=postgresql://postgres:your-password@localhost:5432/example_db\nGDRIVE_ROOT=/home/sammy/GDrive-Mirror\n</code></pre>"},{"location":"cloud/#run-the-analysis","title":"Run the analysis","text":"<p>At this point you should be set up to run the analysis as normal. If you'd like a familiar interface to interact with the code files and terminal, install the <code>Remote-SSH</code> VSCode plugin and use VSCode to connect to the droplet and run commands.</p> <p>At the end of your analysis you'll need to use the FTP to copy the output files from the server back to your local computer.</p>"},{"location":"cloud/#memory-errors","title":"Memory errors","text":"<p>If an analysis script is unexpectedly \"killed\" by the droplet, you may have run out of RAM. If this happens, configure the droplet's swap usage by following this tutorial from Digital Ocean.</p> <p>You can keep an eye on RAM, swap, CPU, and running processes by logging in to the droplet and running <code>htop</code>.</p>"},{"location":"database/","title":"Prepare the Database","text":""},{"location":"database/#import-the-starter-datasets-from-the-web","title":"Import the starter datasets from the web","text":"<p>Import the sidewalk and OpenStreetMap networks as well as a few other datasets from DVRPC's GIS server:</p> <pre><code>make prepare-initial-database\n</code></pre>"},{"location":"database/#import-the-additional-datasets-from-google-drive","title":"Import the additional datasets from Google Drive","text":"<p>There are a number of shapefiles necessary for some of the analysis configurations that have been added to a shared Google Drive folder.</p> <p>These files are organized under sub-folders within <code>Shared drives/network-routing-repo-data/data/inputs</code>. Make sure you have this folder synced to your computer with all files available offline.</p> <p>To import these files and create the network nodes necessary for the routing analyses, run the following command:</p> <pre><code>make prepare-for-analysis\n</code></pre>"},{"location":"installation/","title":"Installation &amp; Setup","text":""},{"location":"installation/#clone-the-git-repo-and-build-with-local-files","title":"Clone the <code>git</code> repo and build with local files","text":"<p>Clone the repo, <code>cd</code> into the new folder, and build the Python environment with <code>conda</code>.</p> <pre><code>git clone https://github.com/dvrpc/network-routing.git\ncd network-routing\nconda env create -f environment.yml\n</code></pre> <p>After installing the dependencies the <code>environment.yml</code> file will install the <code>network_routing</code> package via <code>pip install --editable .</code> which will allow you to modify the source code locally.</p>"},{"location":"installation/#create-the-postgis-database","title":"Create the PostGIS database","text":"<p>Using whatever tooling you're most comfortable with, create a PostgreSQL database locally and enable PostGIS within it by running:</p> <pre><code>CREATE EXTENSION postgis;\n</code></pre>"},{"location":"installation/#create-configuration-file","title":"Create configuration file","text":"<p>This analysis requires a file named <code>.env</code> that defines the Postgres database URL and a Google Drive folder path.</p> <p>You can place this file wherever you intend to run the analysis from. It should look like this:</p> <pre><code>DATABASE_URL=postgresql://username:password@host:5432/database_name\nGDRIVE_ROOT=/Volumes/GoogleDrive\nPG_DUMP_PATH=pg_dump\n</code></pre> <p>If the <code>pg_dump</code> executable in your Python environment does not match the Postgres server version you're running, you'll need to define a full path to the specific version of <code>pg_dump</code> that matches your server version. For example, on a Mac with postgres.app installed, you should set <code>PG_DUMP_PATH</code> to this:</p> <pre><code>PG_DUMP_PATH=/Applications/Postgres.app/Contents/Versions/latest/bin/pg_dump\n</code></pre>"},{"location":"installation/#activate-the-virtual-environment","title":"Activate the virtual environment","text":"<p>Although you only need to install and create the <code>.env</code> file once, you'll always need to activate the environment before running any commands.</p> conda <pre><code>conda activate network_routing\n</code></pre> venv <pre><code>source ./env/bin/activate\n</code></pre>"},{"location":"methodology/","title":"Network Analysis Methodology","text":""},{"location":"methodology/#introduction","title":"Introduction","text":"<p>The core logic for the accessibility analysis is encapsulated within the <code>RoutableNetwork</code> class.</p> <p>This code builds upon the <code>pandana</code> library, adding the ability to read data inputs from PostgreSQL and to save analysis results back to PostgreSQL.</p>"},{"location":"methodology/#workflow","title":"Workflow","text":""},{"location":"methodology/#step-1-setup","title":"Step 1: Setup","text":"<ul> <li>Get a list of all unique POI categories</li> <li>Assign start/end node ID values to each segment</li> <li>Calculate the travel time cost to traverse each segment</li> </ul>"},{"location":"methodology/#step-2-construct-network","title":"Step 2: Construct network","text":"<ul> <li>Extract edge and node geodataframes from SQL</li> <li>Build a <code>pandana.Network()</code> and precompute it</li> </ul>"},{"location":"methodology/#step-3-analyze-pois","title":"Step 3: Analyze POIs","text":"<ul> <li>Calculate each POI category by adding the POI category to the network and computing distances to the N-nearest POIs for every node in the network</li> <li>Combine the results of each single POI analysis into one table</li> <li>Save table to database and generate geotable with results tied to geometries</li> <li>Clean up by moving results to a new schema and deleting temp QAQC tables</li> </ul>"},{"location":"methodology/#python-usage","title":"Python usage","text":"<p>To use this code directly in a Python process, import the <code>RoutableNetwork</code> class from the <code>network_routing</code> module, instantiate the class with keyword arguments, and run the command to compute every POI result into a single output table in postgres:</p> <pre><code>from network_routing import pg_db_connection\nfrom network_routing.accessibility.routable_network import RoutableNetwork\n\ndb = pg_db_connection()\n\narguments = {\n    \"edge_table_name\": \"pedestriannetwork_lines\",\n    \"node_table_name\": \"nodes_for_sidewalks\",\n    \"node_id_column\": \"sw_node_id\",\n    \"poi_table_name\": \"regional_transit_with_accessscore\",\n    \"poi_id_column\": \"category\",\n    \"output_table_name\": \"regional_transit_stops\",\n    \"output_schema\": \"sw_defaults\",\n    \"max_minutes\": 120,\n    \"poi_match_threshold\": 152,\n}\n\nnet = RoutableNetwork(db, **arguments)\nnet.compute_every_poi_into_one_postgres_table()\n</code></pre>"},{"location":"other_analyses/","title":"Other Analyses","text":""},{"location":"other_analyses/#network-analysis-configurations","title":"Network analysis configurations","text":"<p>There are a number of other accessibility analyses defined within</p> <p><code>./network_routing/accessibility/cli.py</code></p> <p>These can be run in a one-off fashion using the <code>access</code> command. For example, to execute the analysis for the PART project you can run:</p> <pre><code>access part-sidewalk\naccess part-osm\n</code></pre> <p>Each of these configurations defines the network edge table, the Point-of-Interest table, and other analysis parameters like search distance, snap tolerance, and output schema and table names.</p>"},{"location":"other_analyses/#draw-missing-sidewalk-segments","title":"Draw missing sidewalk segments","text":"<p>To draw the missing sidewalks (i.e. \"gaps\") run the following command:</p> <pre><code>improvements draw-missing-network-links COUNTYNAME\n</code></pre> <p>Replace <code>COUNTYNAME</code> with the name of the county you want to analyze, like \"<code>Montgomery</code>\", \"<code>Camden</code>\", etc.</p> <p>Note: this command can only be run after first running <code>gaps classify-osm-sw-coverage</code></p> <p>The result of this script is a set of lines that overshoots what's needed. To clean this dataset up, run the following command:</p> <pre><code>improvements feature-engineering --erase\n</code></pre> <p>After running this script, the outputs can be exported with:</p> <pre><code>db export-geojson regional_gaps\n</code></pre>"},{"location":"sidewalk_gap_analysis/","title":"Sidewalk Gap Analysis","text":""},{"location":"sidewalk_gap_analysis/#generate-data-for-the-sidewalk-gap-analysis-explorer","title":"Generate data for the Sidewalk Gap Analysis Explorer","text":"<p>All of the analyses and export procedures needed for the Sidewalk Gap Analysis Explorer can be run with the following command:</p> <pre><code>make sidewalk-gaps-map\n</code></pre> <p>This command will run for a number of hours, and bundles a number of commands in sequence.</p>"},{"location":"sidewalk_gap_analysis/#bundled-commands","title":"Bundled Commands","text":"<p>Identify the sidewalk coverage of every OSM centerline:</p> <p><code>gaps classify-osm-sw-coverage</code></p> <p>Clean up the OSM feature tags to help filter the output for the webmap:</p> <p><code>gaps scrub-osm-tags</code></p> <p>Identify portions of the sidewalk network that form discrete \"islands\":</p> <p><code>gaps identify-islands</code></p> <p>Run a routing analysis to transit stops of all modes across the region:</p> <p><code>access sw-default</code></p> <p>Run a routing analysis to all public and private schools across the region:</p> <p><code>access eta-schools</code></p> <p>Export geojson data for the school, transit, island, and centerline analyses:</p> <p><code>db export-geojson gaps</code></p> <p>Identify the walksheds around every rail station across the region using the sidewalk network and then the OpenStreetMap network:</p> <p><code>access sw-access-score</code></p> <p><code>access osm-access-score</code></p> <p>Turn the OSM and sidewalk rail station analyses into polygon isochrones:</p> <p><code>gaps isochrones-accessscore</code></p> <p>Export the rail station isochrones to geojson:</p> <p><code>db export-geojson accessscore</code></p> <p>Turn the exported geojson files into a single vector tileset for the web map:</p> <p><code>db make-vector-tiles gaps sidewalk_gap_analysis</code></p>"}]}